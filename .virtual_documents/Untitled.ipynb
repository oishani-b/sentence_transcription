


import pandas as pd
import numpy as np


wer_top_df = pd.read_csv('wer_oishani_top.csv')
wer_bottom_df = pd.read_csv('wer_oishani_bottom.csv')
wer_df = pd.concat([wer_top_df, wer_bottom_df])
wer_df.head(3)


wer_df.shape


wer_df.drop(['Unnamed: 0.2', 'Unnamed: 0.1'], axis=1)
wer_df.shape


wer_df_high_large = wer_df[wer_df['large_wer']>0.5]
wer_df_high_large.head(3)


wer_df_high_large.shape


wer_df_high_large.value_counts(['participant_id'])


# dropping participant t2 due to very high WER - double checking others
wer_t2_drop = wer_df_high_large[wer_df_high_large['participant_id'] != 't2']
wer_t2_drop.shape


wer_t2_drop.value_counts(['participant_id'])


# let's start by looking at t54
high_wer_t54 = wer_t2_drop[wer_t2_drop['participant_id'] == 't54']
high_wer_t54


# lots of NaN values - let's see how many
high_wer_t54['base_preproc'].isna().count()


# all of them are NaN! let's drop this participant as well for now
wer_t2_t54_drop = wer_t2_drop[wer_t2_drop['participant_id'] != 't54']
wer_t2_t54_drop.shape


# let's look at t14 next
high_wer_t14 = wer_t2_t54_drop[wer_t2_t54_drop['participant_id'] == 't14']
high_wer_t14


# maybe we have a lot of na's here too?
high_wer_t14.isna().count()


# yes we do! let's drop them and see how many values we'll still be left with
# we can use a NaN in base_preproc as our heuristic here
high_wer_t14_na_drop = high_wer_t14[high_wer_t14['base_preproc'].isna() == False]
high_wer_t14_na_drop.shape


# interesting, so we lost only one row of NAs. let's download these files to inspect the transcripts closely
# we can see if there is an easily corrected issue
high_wer_t14_na_drop.to_csv('t14_wer.csv')


# from looking at the CSV, a good heuristic for speaker mistakes seems to be the large WER
# let's filter this to be the WER higher than 1 gets removed
high_wer_t14_mistake_drop = high_wer_t14_na_drop[high_wer_t14_na_drop['large_wer'] <= 1]
high_wer_t14_mistake_drop


# we lost 12 rows, but these seem to be Whisper issues more than speaker errors
# let's add these changes back into the dataset we've been cleaning by filtering for 
# participant t14's nan and high error values


wer_t2_t54_t14 = wer_t2_t54_drop[(wer_t2_t54_drop['participant_id'] != 't14') | (wer_t2_t54_drop['large_wer'] <= 1)]
# != t14 gives all non t14 participants
# the | or condition applies only to t14, so we keep any WER less than/equal to 1 for t14
wer_t2_t54_t14.shape


# we lose 12 datapoints as expected
# next, let's look at remaining high error participants
wer_t2_t54_t14.value_counts(['participant_id'])


# let's look at t22
high_wer_t22 = wer_t2_t54_t14[wer_t2_t54_t14['participant_id'] == 't22']
high_wer_t22



